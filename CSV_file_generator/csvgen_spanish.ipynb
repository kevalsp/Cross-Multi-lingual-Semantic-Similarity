{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files created!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import numbers\n",
    "import nltk\n",
    "import csv, random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "fd = open(\"/Users/kevalpaida/Text Mining Project/Data/totc_spa.txt\")\n",
    "corpus = fd.read()\n",
    "\n",
    "cust_sent_tokenize = PunktSentenceTokenizer(corpus)\n",
    "corpus_arr = cust_sent_tokenize.tokenize(corpus)\n",
    "\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def tagged_to_synset_en(word):\n",
    "    try:\n",
    "        lemmas = wordnet.lemmas(word, lang=\"spa\")\n",
    "        hypernyms = lemmas[0].synset().hypernyms()\n",
    "        return hypernyms\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def sentence_similarity_en(sentence, mth):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    sentence2 = []\n",
    "    sentence1 = word_tokenize(sentence)\n",
    "    # random.shuffle(corpus_arr)\n",
    "    for w in corpus_arr[:20]:\n",
    "        sentence2 = sentence2 + word_tokenize(w)\n",
    "\n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset_en(tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset_en(tagged_word) for tagged_word in sentence2]\n",
    "\n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss[0] for ss in synsets1 if ss]\n",
    "    synsets2 = [ss[0] for ss in synsets2 if ss]\n",
    "\n",
    "    score, count = 0.0, 0\n",
    "\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        if mth == 'path':\n",
    "            scores = [synset.path_similarity(ss) for ss in synsets2]\n",
    "        elif mth == 'wu':\n",
    "            scores = [synset.wup_similarity(ss) for ss in synsets2]\n",
    "\n",
    "        numscore = [x for x in scores if isinstance(x, numbers.Number)]\n",
    "        if len(numscore) != 0:\n",
    "            best_score = max(numscore)\n",
    "            # Check that the similarity could have been computed\n",
    "            if best_score is not None:\n",
    "                score += best_score\n",
    "\n",
    "        else:\n",
    "            score += 0\n",
    "        count += 1\n",
    "\n",
    "    # Average the values\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        score /= count\n",
    "        return score\n",
    "\n",
    "# convert percentage into scores\n",
    "def score_generator(score):\n",
    "    if score >= 0 and score <= 0.2:\n",
    "        return 0\n",
    "    elif score > 0.2 and score <= 0.4:\n",
    "        return 1\n",
    "    elif score > 0.4 and score <= 0.6:\n",
    "        return 2\n",
    "    elif score > 0.6 and score <= 0.8:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# write into CSV\n",
    "def model_gen(sentences):\n",
    "    with open(path, 'w') as csvfile:\n",
    "        fieldnames = ['Sentences', 'Score']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for sentence in sentences:\n",
    "            path_sem_sim = sentence_similarity_en(sentence, 'wu')\n",
    "            writer.writerow({'Sentences': sentence, 'Score': score_generator(path_sem_sim)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_gen(corpus_arr)\n",
    "\n",
    "print(\"CSV files created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
